{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPIfhfSu38BwMRyDPfAIbhh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fouziasharkar/Programming-and-Systems-Development-/blob/main/Pytorch_Basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psMlPBUqqlC-",
        "outputId": "84bfccb8-8be1-45fa-c60c-ed569117f264"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.0+cu126\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print (torch.__version__)\n",
        "\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Activation function**\n",
        "\n",
        "An activation function decides whether a neuron should be “activated” or not — it adds non-linearity to the model so it can learn complex patterns.\n",
        "\n",
        "**🧠 Without it:**\n",
        "\n",
        "The whole network behaves like a simple linear equation (no matter how many layers you add).\n",
        "\n",
        "**🧩 With it:**\n",
        "\n",
        "The model can learn curves, interactions, and complex relationships in data — enabling deep learning.\n",
        "\n",
        "**✅ Common activation functions:**\n",
        "\n",
        "**ReLU:** (Rectified Linear Unit): f(x) = max(0, x) → fast and widely used.\n",
        "\n",
        "**Sigmoid:** squashes values between 0 and 1 → often used in binary classification.\n",
        "\n",
        "**Tanh:** squashes values between -1 and 1 → centered around zero.\n",
        "\n",
        "**Softmax:** converts outputs into probabilities that sum to 1 (used in multi-class classification).\n",
        "\n",
        "📘 In short:\n",
        "Activation functions make neural networks non-linear, powerful, and capable of learning real-world data patterns."
      ],
      "metadata": {
        "id": "DNpXZ3BcFX2s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Weight**\n",
        "\n",
        "🧩 **Definition**\n",
        "\n",
        "A weight is a learnable number (parameter) that controls how strongly an input affects a neuron’s output.\n",
        "\n",
        "It’s like an adjustable “importance” score for each input feature.\n",
        "\n",
        "🧠 **Mathematically**\n",
        "\n",
        "For one neuron:\n",
        "\n",
        "                    y=w1​x1​+w2​x2​+w3​x3​+b\n",
        "\n",
        "\n",
        "*   x₁, x₂, x₃ → inputs (like pixel values)\n",
        "*   w₁, w₂, w₃ → weights (how much each input matters)\n",
        "*   b → bias (shifts the output)\n",
        "*   y → result passed to activation function\n",
        "\n",
        "So, each weight decides how much influence its corresponding input has on the output.\n",
        "\n",
        "💡 **Intuition**\n",
        "\n",
        "Think of weights as knobs that your model keeps adjusting until it predicts correctly.\n",
        "The higher a weight → the more that input contributes to the decision."
      ],
      "metadata": {
        "id": "AY0U7igTKA4j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bias**\n",
        "\n",
        "👉 Bias helps the model learn even when all inputs are zero.\n",
        "\n",
        "It acts like a shift in the output of a neuron or layer, allowing the model to fit the data better by adjusting the position of the activation function.\n",
        "\n",
        "Mathematically:\n",
        "\n",
        "                         y=Wx+b\n",
        "\n",
        "\n",
        "*   W (weights) controls the slope — how input affects output.\n",
        "*   b (bias) shifts the output up or down — letting the model learn patterns that don’t pass through the origin (0,0).\n",
        "\n",
        "\n",
        "\n",
        "**🧠 Without bias:** the neuron’s output is always zero when inputs are zero → limits flexibility and accuracy.\n",
        "\n",
        "🧩 **With bias:** the model can better represent complex relationships in data."
      ],
      "metadata": {
        "id": "jaDbLd6kLvGD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Epoch**\n",
        "\n",
        "🧩 **Definition**\n",
        "\n",
        "An epoch means one complete pass of the entire training dataset through the neural network.\n",
        "\n",
        "So if you have 60,000 images in MNIST:\n",
        "👉 1 epoch = model sees all 60,000 images once.\n",
        "\n",
        "⚙️ **Why use multiple epochs?**\n",
        "\n",
        "Because one pass isn’t enough for the model to learn all the patterns.\n",
        "You usually train for many epochs so the model keeps adjusting its weights and biases until the error (loss) gets small.\n",
        "\n",
        "🧠 **Analogy**\n",
        "\n",
        "Think of an epoch like a “study round.”\n",
        "\n",
        "After 1 round: model has seen everything once.\n",
        "\n",
        "After several rounds: it has practiced and refined what it learned.\n",
        "\n",
        "✅ **In short:**\n",
        "\n",
        "Epoch = one full training cycle over the entire dataset.\n",
        "More epochs = more learning (until the model starts to overfit)."
      ],
      "metadata": {
        "id": "EychlCrcTML-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kJl179wZSSkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Autograd**\n",
        "\n",
        "🧩 **Definition**\n",
        "\n",
        "Think of it as PyTorch’s “memory” of operations —\n",
        "it records every tensor operation in a computation graph, and when you call .backward(), it traces that graph to compute gradients.\n",
        "\n",
        "\n",
        "**Inside your training loop, every time you:**\n",
        "\n",
        "**loss.backward()**  # ← this triggers autograd\n",
        "\n",
        "**optimizer.step()**  # ← this uses the gradients to update weights"
      ],
      "metadata": {
        "id": "dTHY2M-VR9m9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a model for one neural Network\n",
        "# five input - > one output\n",
        "\n",
        "class AmarModel(nn.Module): # nn.model class the inherit\n",
        "  def __init__(self, input_features):\n",
        "    super().__init__() #parent class constructor invoking\n",
        "    self.linear = nn.Linear(input_features, 1) # input layer\n",
        "    self.sigmoid = nn.Sigmoid() # activation function\n",
        "\n",
        "\n",
        "  def forward(self, features):\n",
        "    out = self.linear(features)\n",
        "    out = self.sigmoid(out)\n",
        "\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "ORlbiWEisF0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating dataset\n",
        "\n",
        "features = torch.rand(10,5) # 10 rows, 5 column\n",
        "\n",
        "#object of AmarModel\n",
        "model = AmarModel(features.shape[1])\n",
        "\n",
        "# nn model e evabe forward function call kora jay\n",
        "#karon ei model r object call korar sathe sathe forward function auto call hoy\n",
        "model(features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPIc3PbLsF3j",
        "outputId": "f61a763d-c9fa-4c14-f252-ccac9e9e77c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.5876],\n",
              "        [0.5282],\n",
              "        [0.5723],\n",
              "        [0.5345],\n",
              "        [0.4124],\n",
              "        [0.5101],\n",
              "        [0.4826],\n",
              "        [0.4927],\n",
              "        [0.4434],\n",
              "        [0.4836]], grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "etbWlmL6sF6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "obH1cbCMsF8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sayYaEPisF_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U6PR7CnTsGCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-G5aXgXksGFk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}